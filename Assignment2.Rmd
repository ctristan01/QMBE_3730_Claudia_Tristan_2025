---
title: "Assignment 2"
output: pdf_document
date: "2025-02-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caTools)
getwd()
setwd("/Users/claudiatristan/Desktop/QMBE_3730_Claudia_Tristan_2025")
admit <- read_csv("admit.csv")
```


```{r}
view(admit)
```


```{r}
summary(admit)
```
#comments
Admission looks imbalanced, GPA and GRE scores seem to be associated with higher chances of admission, these are the most important predictors.

For GRE our mean and median have a symmetric distribution and not a possible chance of a lot of outliers. there might be a possible light right skew due to the 660 extending towards the 800 but overall it seems like a normal distribution 


```{r}
colSums(is.na(admit))
```

#comments
we see no missing values

```{r}
admit$admit <- as.factor(admit$admit)
```

```{r}
table(admit$admit)
prop.table(table(admit$admit))
```
#comments
The data set is imbalanced, with more rejections than acceptances.

```{r}
set.seed(1)
split <- sample.split(admit$admit, SplitRatio = 0.7)
train_data <- subset(admit, split == TRUE)
test_data <- subset(admit, split == FALSE)
```

```{r}
dim(train_data)
dim(test_data)
```

```{r}
log_model <- glm(admit ~ ., data = train_data, family = binomial)
```

```{r}
summary(log_model)
```
#comments
GPA and Rank are the most significant values, higher GPA improves your chances of getting accepted and the lower the rank of the school the lower the admission chances.
Gre does not seem too significant with a value of 0.518 and does not influence the chances of admission.

```{r}
pred_probs <- predict(log_model, test_data, type = "response")
```

```{r}
pred_classes <- ifelse(pred_probs > 0.5, 1, 0)
pred_classes <- as.factor(pred_classes)
```

```{r}
head(pred_probs)
head(pred_classes)
```
#comments

we have a 50% first applicant admitted, second applicant 41% who was not, third applicant with a 65% who was admitted and others below 50% who were not.

```{r}
do.call(rbind, Map(data.frame, predicted_classes = pred_classes, admit = test_data$admit))
```
#comments
we can see the ones that were correctly predicted and the ones the weren't in which 8 were correctly predicted out of the 10, as we have 120 rows we can see we are predicting correctly more.

```{r}
conf_matrix <- table(Predicted = pred_classes, Actual = test_data$admit)
conf_matrix
```
#comments

we have 77 true negative, 5 false negative, 29 false positive, and 9 true positive. we have a high false positives and for our recall it seems we capture most admitted students but miss by 5.

```{r}
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(conf_matrix)
print(paste("Accuracy:", round(accuracy, 4)))
```
our model is 71% accurate

```{r}
ggplot(test_data, aes(x = gre, y = as.numeric(as.character(admit)), color = as.factor(pred_classes))) +
  geom_point(size = 3) +
  labs(title = "Predicted vs Actual Admission Status",
       x = "gre Score",
       y = "Admission Status (0 = Not Admitted, 1 = Admitted)") +
  scale_color_manual(values = c("red", "blue"), name = "Prediction")
```


most of the lower GRE scores are correctly predicted as the not admitted red dots. and the higher scores seem to have mix predictions. a few false negatives and positives seem to be in our data, knowing our data is only 71% percent accurate a higher accuracy would make fewer mistakes but accuracy itself can't be the only reason for imbalance.

